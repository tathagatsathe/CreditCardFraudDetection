{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Deploying the Fraud Detection Model\n",
    "\n",
    "In this notebook, we will take the outputs from the Processing Job in the previous step and use it and train and deploy an XGBoost model. Our historic transaction dataset is initially comprised of data like timestamp, card number, and transaction amount and we enriched each transaction with features about that card number's recent history, including:\n",
    "\n",
    "- `num_trans_last_10m`\n",
    "- `num_trans_last_1w`\n",
    "- `avg_amt_last_10m`\n",
    "- `avg_amt_last_1w`\n",
    "\n",
    "Individual card numbers may have radically different spending patterns, so we will want to use normalized ratio features to train our XGBoost model to detect fraud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DIR = './data'\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "PREFIX = 'training'\n",
    "\n",
    "# sagemaker_role = sagemaker.get_execution_role()\n",
    "sagemaker_role = 'arn:aws:iam::499469212086:role/sagemaker_creditcard'\n",
    "s3_client = boto3.Session().client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the results of the SageMaker Processing Job ran in the previous step into a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{LOCAL_DIR}/aggregated/processing_output.csv')\n",
    "#df.dropna(inplace=True)\n",
    "df['cc_num'] = df['cc_num'].astype(np.int64)\n",
    "df['fraud_label'] = df['fraud_label'].astype(np.int64)\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split DataFrame into Train & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artifically generated dataset contains transactions from `2020-01-01` to `2020-06-01`. We will create a training and validation set out of transactions from `2020-01-15` and `2020-05-15`, discarding the first two weeks in order for our aggregated features to have built up sufficient history for cards and leaving the last two weeks as a holdout test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start = '2020-01-15'\n",
    "training_end = '2020-05-15'\n",
    "\n",
    "training_df = df[(df.datetime > training_start) & (df.datetime < training_end)]\n",
    "test_df = df[df.datetime >= training_end]\n",
    "\n",
    "test_df.to_csv(f'{LOCAL_DIR}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we now have lots of information about each transaction in our training dataset, we don't want to pass everything as features to the XGBoost algorithm for training because some elements are not useful for detecting fraud or creating a performant model:\n",
    "- A transaction ID and timestamp is unique to the transaction and never seen again. \n",
    "- A card number, if included in the feature set at all, should be a categorical variable. But we don't want our model to learn that specific card numbers are associated with fraud as this might lead to our system blocking genuine behaviour. Instead we should only have the model learn to detect shifting patterns in a card's spending history. \n",
    "- Individual card numbers may have radically different spending patterns, so we will want to use normalized ratio features to train our XGBoost model to detect fraud. \n",
    "\n",
    "Given all of the above, we drop all columns except for the normalised ratio features and transaction amount from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202185/3217280059.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_df.drop(['tid','datetime','cc_num','num_trans_last_10m', 'avg_amt_last_10m',\n"
     ]
    }
   ],
   "source": [
    "training_df.drop(['tid','datetime','cc_num','num_trans_last_10m', 'avg_amt_last_10m',\n",
    "       'num_trans_last_1w', 'avg_amt_last_1w'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) requires the label to be the first column in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>amount</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>362.22</td>\n",
       "      <td>0.535936</td>\n",
       "      <td>0.535936</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>356.44</td>\n",
       "      <td>0.522487</td>\n",
       "      <td>0.522487</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>30.04</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.016441</td>\n",
       "      <td>0.016441</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fraud_label  amount  amt_ratio1  amt_ratio2  count_ratio\n",
       "46            0  362.22    0.535936    0.535936     0.052632\n",
       "47            0    0.93    0.001401    0.001401     0.052632\n",
       "48            0  356.44    0.522487    0.522487     0.052632\n",
       "49            0   30.04    0.039566    0.039566     0.058824\n",
       "50            0   11.80    0.016441    0.016441     0.055556"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = training_df[['fraud_label', 'amount', 'amt_ratio1','amt_ratio2','count_ratio']]\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(training_df, test_size=0.3)\n",
    "train.to_csv(f'{LOCAL_DIR}/train.csv', header=False, index=False)\n",
    "val.to_csv(f'{LOCAL_DIR}/val.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train.csv to s3://sagemaker-ap-south-1-499469212086/training/train.csv\n",
      "upload: data/val.csv to s3://sagemaker-ap-south-1-499469212086/training/val.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {LOCAL_DIR}/train.csv s3://{BUCKET}/{PREFIX}/\n",
    "!aws s3 cp {LOCAL_DIR}/val.csv s3://{BUCKET}/{PREFIX}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-05-29-11-18-26-387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-29 11:18:27 Starting - Starting the training job...\n",
      "2023-05-29 11:18:41 Starting - Preparing the instances for training...\n",
      "2023-05-29 11:19:26 Downloading - Downloading input data...\n",
      "2023-05-29 11:19:41 Training - Downloading the training image...\n",
      "2023-05-29 11:20:17 Training - Training image download completed. Training in progress...[2023-05-29 11:20:39.080 ip-10-0-64-173.ap-south-1.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "INFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\n",
      "INFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\n",
      "Returning the value itself\n",
      "INFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\n",
      "INFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\n",
      "INFO:root:Determined delimiter of CSV input is ','\n",
      "INFO:root:Determined delimiter of CSV input is ','\n",
      "INFO:root:Determined delimiter of CSV input is ','\n",
      "INFO:root:Determined delimiter of CSV input is ','\n",
      "INFO:root:Single node training.\n",
      "[2023-05-29 11:20:39.160 ip-10-0-64-173.ap-south-1.compute.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-05-29 11:20:39.161 ip-10-0-64-173.ap-south-1.compute.internal:7 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-05-29 11:20:39.161 ip-10-0-64-173.ap-south-1.compute.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\n",
      "INFO:root:Debug hook created from config\n",
      "[2023-05-29 11:20:39.161 ip-10-0-64-173.ap-south-1.compute.internal:7 INFO hook.py:253] Saving to /opt/ml/output/tensors\n",
      "[2023-05-29 11:20:39.162 ip-10-0-64-173.ap-south-1.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "INFO:root:Train matrix has 29922 rows and 4 columns\n",
      "INFO:root:Validation matrix has 12824 rows\n",
      "[0]#011train-error:0.00130#011validation-error:0.00117\n",
      "[2023-05-29 11:20:39.182 ip-10-0-64-173.ap-south-1.compute.internal:7 INFO hook.py:413] Monitoring the collections: metrics\n",
      "[2023-05-29 11:20:39.184 ip-10-0-64-173.ap-south-1.compute.internal:7 INFO hook.py:476] Hook is writing from the hook with pid: 7\n",
      "[1]#011train-error:0.00130#011validation-error:0.00117\n",
      "[2]#011train-error:0.00130#011validation-error:0.00117\n",
      "[3]#011train-error:0.00130#011validation-error:0.00117\n",
      "[4]#011train-error:0.00130#011validation-error:0.00117\n",
      "[5]#011train-error:0.00130#011validation-error:0.00117\n",
      "[6]#011train-error:0.00130#011validation-error:0.00117\n",
      "[7]#011train-error:0.00130#011validation-error:0.00117\n",
      "[8]#011train-error:0.00130#011validation-error:0.00117\n",
      "[9]#011train-error:0.00130#011validation-error:0.00117\n",
      "[10]#011train-error:0.00130#011validation-error:0.00117\n",
      "[11]#011train-error:0.00130#011validation-error:0.00117\n",
      "[12]#011train-error:0.00130#011validation-error:0.00117\n",
      "[13]#011train-error:0.00130#011validation-error:0.00117\n",
      "[14]#011train-error:0.00130#011validation-error:0.00117\n",
      "[15]#011train-error:0.00130#011validation-error:0.00117\n",
      "[16]#011train-error:0.00130#011validation-error:0.00117\n",
      "[17]#011train-error:0.00130#011validation-error:0.00117\n",
      "[18]#011train-error:0.00130#011validation-error:0.00117\n",
      "[19]#011train-error:0.00130#011validation-error:0.00117\n",
      "[20]#011train-error:0.00130#011validation-error:0.00117\n",
      "[21]#011train-error:0.00130#011validation-error:0.00117\n",
      "[22]#011train-error:0.00130#011validation-error:0.00117\n",
      "[23]#011train-error:0.00130#011validation-error:0.00117\n",
      "[24]#011train-error:0.00130#011validation-error:0.00117\n",
      "[25]#011train-error:0.00130#011validation-error:0.00117\n",
      "[26]#011train-error:0.00130#011validation-error:0.00117\n",
      "[27]#011train-error:0.00130#011validation-error:0.00117\n",
      "[28]#011train-error:0.00130#011validation-error:0.00117\n",
      "[29]#011train-error:0.00130#011validation-error:0.00117\n",
      "[30]#011train-error:0.00130#011validation-error:0.00117\n",
      "[31]#011train-error:0.00130#011validation-error:0.00117\n",
      "[32]#011train-error:0.00134#011validation-error:0.00117\n",
      "[33]#011train-error:0.00134#011validation-error:0.00117\n",
      "[34]#011train-error:0.00134#011validation-error:0.00117\n",
      "[35]#011train-error:0.00134#011validation-error:0.00117\n",
      "[36]#011train-error:0.00134#011validation-error:0.00117\n",
      "[37]#011train-error:0.00134#011validation-error:0.00117\n",
      "[38]#011train-error:0.00134#011validation-error:0.00117\n",
      "[39]#011train-error:0.00134#011validation-error:0.00117\n",
      "[40]#011train-error:0.00134#011validation-error:0.00117\n",
      "[41]#011train-error:0.00134#011validation-error:0.00117\n",
      "[42]#011train-error:0.00134#011validation-error:0.00117\n",
      "[43]#011train-error:0.00134#011validation-error:0.00117\n",
      "[44]#011train-error:0.00134#011validation-error:0.00117\n",
      "[45]#011train-error:0.00134#011validation-error:0.00117\n",
      "[46]#011train-error:0.00134#011validation-error:0.00117\n",
      "[47]#011train-error:0.00134#011validation-error:0.00117\n",
      "[48]#011train-error:0.00134#011validation-error:0.00117\n",
      "[49]#011train-error:0.00134#011validation-error:0.00117\n",
      "[50]#011train-error:0.00134#011validation-error:0.00117\n",
      "[51]#011train-error:0.00134#011validation-error:0.00117\n",
      "[52]#011train-error:0.00134#011validation-error:0.00117\n",
      "[53]#011train-error:0.00134#011validation-error:0.00117\n",
      "[54]#011train-error:0.00130#011validation-error:0.00117\n",
      "[55]#011train-error:0.00130#011validation-error:0.00117\n",
      "[56]#011train-error:0.00134#011validation-error:0.00117\n",
      "[57]#011train-error:0.00134#011validation-error:0.00117\n",
      "[58]#011train-error:0.00134#011validation-error:0.00117\n",
      "[59]#011train-error:0.00134#011validation-error:0.00117\n",
      "[60]#011train-error:0.00134#011validation-error:0.00117\n",
      "[61]#011train-error:0.00134#011validation-error:0.00117\n",
      "[62]#011train-error:0.00134#011validation-error:0.00117\n",
      "[63]#011train-error:0.00130#011validation-error:0.00117\n",
      "[64]#011train-error:0.00134#011validation-error:0.00117\n",
      "[65]#011train-error:0.00134#011validation-error:0.00117\n",
      "[66]#011train-error:0.00134#011validation-error:0.00117\n",
      "[67]#011train-error:0.00134#011validation-error:0.00117\n",
      "[68]#011train-error:0.00134#011validation-error:0.00117\n",
      "[69]#011train-error:0.00134#011validation-error:0.00117\n",
      "[70]#011train-error:0.00134#011validation-error:0.00117\n",
      "[71]#011train-error:0.00134#011validation-error:0.00117\n",
      "[72]#011train-error:0.00134#011validation-error:0.00117\n",
      "[73]#011train-error:0.00134#011validation-error:0.00117\n",
      "[74]#011train-error:0.00134#011validation-error:0.00117\n",
      "[75]#011train-error:0.00134#011validation-error:0.00117\n",
      "[76]#011train-error:0.00134#011validation-error:0.00117\n",
      "[77]#011train-error:0.00134#011validation-error:0.00117\n",
      "[78]#011train-error:0.00134#011validation-error:0.00117\n",
      "[79]#011train-error:0.00134#011validation-error:0.00117\n",
      "[80]#011train-error:0.00134#011validation-error:0.00117\n",
      "[81]#011train-error:0.00134#011validation-error:0.00117\n",
      "[82]#011train-error:0.00134#011validation-error:0.00117\n",
      "[83]#011train-error:0.00134#011validation-error:0.00117\n",
      "[84]#011train-error:0.00134#011validation-error:0.00117\n",
      "[85]#011train-error:0.00134#011validation-error:0.00117\n",
      "[86]#011train-error:0.00130#011validation-error:0.00117\n",
      "[87]#011train-error:0.00127#011validation-error:0.00117\n",
      "[88]#011train-error:0.00127#011validation-error:0.00117\n",
      "[89]#011train-error:0.00130#011validation-error:0.00117\n",
      "[90]#011train-error:0.00127#011validation-error:0.00117\n",
      "[91]#011train-error:0.00127#011validation-error:0.00117\n",
      "[92]#011train-error:0.00130#011validation-error:0.00117\n",
      "[93]#011train-error:0.00127#011validation-error:0.00117\n",
      "[94]#011train-error:0.00130#011validation-error:0.00117\n",
      "[95]#011train-error:0.00130#011validation-error:0.00117\n",
      "[96]#011train-error:0.00130#011validation-error:0.00117\n",
      "[97]#011train-error:0.00130#011validation-error:0.00117\n",
      "[98]#011train-error:0.00130#011validation-error:0.00117\n",
      "[99]#011train-error:0.00130#011validation-error:0.00117\n",
      "\n",
      "2023-05-29 11:20:59 Uploading - Uploading generated training model\n",
      "2023-05-29 11:20:59 Completed - Training job completed\n",
      "Training seconds: 92\n",
      "Billable seconds: 92\n"
     ]
    }
   ],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"100\"}\n",
    "\n",
    "output_path = 's3://{}/{}/output'.format(BUCKET, PREFIX)\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", sagemaker.Session().boto_region_name, \"1.2-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role='arn:aws:iam::499469212086:role/sagemaker_creditcard',\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'train.csv'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'val.csv'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would perform hyperparameter tuning before deployment, but for the purposes of this example will deploy the model that resulted from the Training Job directly to a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-05-29-11-21-13-337\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2023-05-29-11-21-13-337\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2023-05-29-11-21-13-337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.t2.medium',\n",
    "    serializer=sagemaker.serializers.CSVSerializer(), wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-xgboost-2023-05-29-11-21-13-337'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name=predictor.endpoint_name\n",
    "#Store the endpoint name for later cleanup \n",
    "%store endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check that our endpoint is working, let's call it directly with a record from our test hold-out set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'64.96,0.1462232574526806,0.1462232574526806,0.0434782608695652'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload_df = test_df.drop(['tid','datetime','cc_num','fraud_label','num_trans_last_10m', 'avg_amt_last_10m',\n",
    "       'num_trans_last_1w', 'avg_amt_last_1w'], axis=1)\n",
    "payload = payload_df.head(1).to_csv(index=False, header=False).strip()\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00040693936171010137"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(predictor.predict(payload).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that the model predicts FRAUD / NOT FRAUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transaction count ratio of: 0.30, fraud score: 0.797\n"
     ]
    }
   ],
   "source": [
    "count_ratio = 0.30\n",
    "payload = f'1.00,1.0,1.0,{count_ratio:.2f}'\n",
    "is_fraud = float(predictor.predict(payload).decode('utf-8'))\n",
    "print(f'With transaction count ratio of: {count_ratio:.2f}, fraud score: {is_fraud:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transaction count ratio of: 0.06, fraud score: 0.002\n"
     ]
    }
   ],
   "source": [
    "count_ratio = 0.06\n",
    "payload = f'1.00,1.0,1.0,{count_ratio:.2f}'\n",
    "is_fraud = float(predictor.predict(payload).decode('utf-8'))\n",
    "print(f'With transaction count ratio of: {count_ratio:.2f}, fraud score: {is_fraud:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "08a7cdee6b48bedb655c5dec0d7984a38f40451508c9ac767696289b7bbe8541"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
